{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy.io as sio\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.util import deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.metrics import adjusted_rand_score as ari\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score as nmi\n",
    "from sklearn.metrics import confusion_matrix, silhouette_score, davies_bouldin_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(dataset):\n",
    "    data = sio.loadmat(os.path.join('data', f'{dataset}.mat'))\n",
    "    features = data['fea'].astype(float)\n",
    "    adj = data['W']\n",
    "    adj = adj.astype(float)\n",
    "\n",
    "    if not sp.issparse(adj):\n",
    "        adj = sp.csc_matrix(adj)\n",
    "    if sp.issparse(features):\n",
    "        features = features.toarray()\n",
    "\n",
    "    labels = data['gnd'].reshape(-1) - 1\n",
    "    n_classes = len(np.unique(labels))\n",
    "    \n",
    "    return adj, features, labels, n_classes\n",
    "\n",
    "def aug_normalized_adjacency(adj, add_loops=True):\n",
    "    if add_loops:\n",
    "        adj = adj + sp.eye(adj.shape[0])\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    row_sum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(row_sum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return d_mat_inv_sqrt.dot(adj).dot(d_mat_inv_sqrt).tocoo()\n",
    "\n",
    "\n",
    "def row_normalize(mx, add_loops=True):\n",
    "    if add_loops:\n",
    "        mx = mx + sp.eye(mx.shape[0])\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "def is_close(a, b, c):\n",
    "    return np.abs(a - b) < c\n",
    "\n",
    "def preprocess_dataset(\n",
    "                       adj, \n",
    "                       features, \n",
    "                       row_norm=True, \n",
    "                       sym_norm=True, \n",
    "                       feat_norm=True, \n",
    "                       tf_idf=False\n",
    "                       ):\n",
    "    if sym_norm:\n",
    "        adj = aug_normalized_adjacency(adj, True)\n",
    "    if row_norm:\n",
    "        adj = row_normalize(adj, True)\n",
    "\n",
    "    if tf_idf:\n",
    "        features = TfidfTransformer().fit_transform(features).toarray()\n",
    "    if feat_norm:\n",
    "        features = normalize(features)\n",
    "    return adj, features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordered_confusion_matrix(y_true, y_pred):\n",
    "    conf_mat = confusion_matrix(y_true, y_pred)\n",
    "    w = np.max(conf_mat) - conf_mat\n",
    "    row_ind, col_ind = linear_sum_assignment(w)\n",
    "    conf_mat = conf_mat[row_ind, :]\n",
    "    conf_mat = conf_mat[:, col_ind]\n",
    "    return conf_mat\n",
    "\n",
    "\n",
    "def clustering_accuracy(y_true, y_pred):\n",
    "    conf_mat = ordered_confusion_matrix(y_true, y_pred)\n",
    "    return np.trace(conf_mat) / np.sum(conf_mat)\n",
    "\n",
    "\n",
    "def clustering_f1_score(y_true, y_pred, **kwargs):\n",
    "    def cmat_to_psuedo_y_true_and_y_pred(cmat):\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        for true_class, row in enumerate(cmat):\n",
    "            for pred_class, elm in enumerate(row):\n",
    "                y_true.extend([true_class] * elm)\n",
    "                y_pred.extend([pred_class] * elm)\n",
    "        return y_true, y_pred\n",
    "\n",
    "    conf_mat = ordered_confusion_matrix(y_true, y_pred)\n",
    "    pseudo_y_true, pseudo_y_pred = cmat_to_psuedo_y_true_and_y_pred(conf_mat)\n",
    "    return f1_score(pseudo_y_true, pseudo_y_pred, **kwargs)\n",
    "\n",
    "\n",
    "def output_metrics(X, y_true, y_pred):\n",
    "    return [\n",
    "        clustering_accuracy(y_true, y_pred),\n",
    "        nmi(y_true, y_pred),\n",
    "        ari(y_true, y_pred),\n",
    "        clustering_f1_score(y_true, y_pred, average='macro'),\n",
    "        davies_bouldin_score(X, y_pred),\n",
    "        silhouette_score(X, y_pred)\n",
    "    ]\n",
    "\n",
    "\n",
    "def print_metrics(metrics_means, metrics_stds, time_mean=None, time_std=None):\n",
    "    if time_mean is not None: print(f'time_mean:{time_mean} ', end='')\n",
    "    print(f'loss_mean:{metrics_means[6]} '\n",
    "          f'acc_mean:{metrics_means[0]} '\n",
    "          f'ari_mean:{metrics_means[2]} '\n",
    "          f'nmi_mean:{metrics_means[1]} '\n",
    "          f'db_mean:{metrics_means[4]} '\n",
    "          f'sil_mean:{metrics_means[5]} '\n",
    "          f'f1_mean:{metrics_means[3]} ', end=' ')\n",
    "\n",
    "    if time_std is not None: print(f'time_std:{time_std} ', end='')\n",
    "    print(f'loss_std:{metrics_stds[6]} '\n",
    "          f'acc_std:{metrics_stds[0]} '\n",
    "          f'ari_std:{metrics_stds[2]} '\n",
    "          f'nmi_std:{metrics_stds[1]} '\n",
    "          f'f1_std:{metrics_stds[3]} '\n",
    "          f'db_std:{metrics_stds[4]} '\n",
    "          f'sil_std:{metrics_stds[5]} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_rule_F(XW, G, k):\n",
    "    F = tf.math.unsorted_segment_mean(XW, G, k)\n",
    "    return F\n",
    "\n",
    "\n",
    "def update_rule_W(X, F, G):\n",
    "    _, U, V = tf.linalg.svd(tf.transpose(X) @ tf.gather(F, G), full_matrices=False)\n",
    "    W = U @ tf.transpose(V)\n",
    "    return W\n",
    "\n",
    "\n",
    "def update_rule_G(XW, F):\n",
    "    centroids_expanded = F[:, None, ...]\n",
    "    distances = tf.reduce_mean(tf.math.squared_difference(XW, centroids_expanded), 2)\n",
    "    G = tf.math.argmin(distances, 0, output_type=tf.dtypes.int32)\n",
    "    return G\n",
    "\n",
    "\n",
    "def init_G_F(XW, k):\n",
    "    km = KMeans(k).fit(XW)\n",
    "    G = km.labels_\n",
    "    F = km.cluster_centers_\n",
    "    return G, F\n",
    "\n",
    "\n",
    "def init_W(X, f):\n",
    "    pca = PCA(f, svd_solver='randomized').fit(X)\n",
    "    W = pca.components_.T\n",
    "    return W\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_loop(X, F, G, W, k, max_iter, tolerance):\n",
    "    losses = tf.TensorArray(tf.float64, size=0, dynamic_size=True)\n",
    "    prev_loss = tf.float64.max\n",
    "\n",
    "    for i in tf.range(max_iter):\n",
    "\n",
    "        W = update_rule_W(X, F, G)\n",
    "        XW = X @ W\n",
    "        G = update_rule_G(XW, F)\n",
    "        F = update_rule_F(XW, G, k)\n",
    "\n",
    "        loss = tf.linalg.norm(X - tf.gather(F @ tf.transpose(W), G))\n",
    "        if prev_loss - loss < tolerance:\n",
    "            break\n",
    "\n",
    "        losses = losses.write(i, loss)\n",
    "        prev_loss = loss\n",
    "\n",
    "    return G, F, W, losses.stack()\n",
    "\n",
    "\n",
    "def optimize(X, k, f, max_iter=30, tolerance=10e-7):\n",
    "    # init G and F\n",
    "    W = init_W(X, f)\n",
    "    G, F = init_G_F(X @ W, k)\n",
    "    G, F, W, loss_history = train_loop(X, F, G, W, k, max_iter, tolerance)\n",
    "\n",
    "    return G, F, W, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Legion\\.conda\\envs\\tf26\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1334: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=11.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Legion\\.conda\\envs\\tf26\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1334: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=11.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Legion\\.conda\\envs\\tf26\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1334: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=11.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Legion\\.conda\\envs\\tf26\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1334: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=11.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Legion\\.conda\\envs\\tf26\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1334: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=11.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Legion\\.conda\\envs\\tf26\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1334: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=11.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Legion\\.conda\\envs\\tf26\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1334: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=11.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Legion\\.conda\\envs\\tf26\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1334: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=11.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Legion\\.conda\\envs\\tf26\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1334: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=11.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Legion\\.conda\\envs\\tf26\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1334: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=11.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Legion\\.conda\\envs\\tf26\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1334: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=11.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Legion\\.conda\\envs\\tf26\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1334: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=11.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Legion\\.conda\\envs\\tf26\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1334: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=11.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Legion\\.conda\\envs\\tf26\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1334: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=11.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Legion\\.conda\\envs\\tf26\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1334: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=11.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Legion\\.conda\\envs\\tf26\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1334: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=11.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Legion\\.conda\\envs\\tf26\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1334: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=11.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Legion\\.conda\\envs\\tf26\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1334: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=11.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Legion\\.conda\\envs\\tf26\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1334: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=11.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Legion\\.conda\\envs\\tf26\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1334: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=11.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_mean:0.5649384617805481 loss_mean:24.872425293728547 acc_mean:0.7362813884785817 ari_mean:0.5034910064428256 nmi_mean:0.5777454313708891 db_mean:0.6918595316858276 sil_mean:0.46830101212875297 f1_mean:0.6974509852858335  time_std:0.6557893343271808 loss_std:0.004770342434718667 acc_std:0.004536652655133224 ari_std:0.006117919887598768 nmi_std:0.005248693315405233 f1_std:0.004308452491614769 db_std:0.014364058377548682 sil_std:0.0004983991202181373 \n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "dataset = 'cora'\n",
    "power = 5\n",
    "runs = 20\n",
    "n_clusters = 0\n",
    "max_iter = 30\n",
    "tolerance = 10e-7\n",
    "\n",
    "\n",
    "# Read the dataset\n",
    "adj, features, labels, n_classes = read_dataset(dataset)\n",
    "if n_clusters == 0: n_clusters = n_classes\n",
    "\n",
    "# Process the dataset\n",
    "tf_idf = (dataset == 'cora' or dataset == 'citeseer') # normalize binary word datasets\n",
    "norm_adj, features = preprocess_dataset(adj, features, tf_idf=tf_idf)\n",
    "\n",
    "\n",
    "run_metrics = []\n",
    "times = []\n",
    "\n",
    "X = features\n",
    "\n",
    "for run in range(runs):\n",
    "    features = X\n",
    "    t0 = time.time()\n",
    "    for _ in range(power):\n",
    "        features = norm_adj @ features\n",
    "\n",
    "    G, F, W, losses = optimize(features, n_clusters, n_clusters,\n",
    "                               max_iter=max_iter, tolerance=tolerance)\n",
    "    time_it_took = time.time() - t0\n",
    "    metrics = output_metrics(features @ W, labels, G)\n",
    "    run_metrics.append(metrics + [losses[-1]])\n",
    "    times.append(time_it_took)\n",
    "\n",
    "print_metrics(np.mean(run_metrics, 0), np.std(run_metrics, 0), np.mean(times), np.std(times))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tf26')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4b521e29a846470c96e928a1c4aafac58a12234cdaa98f9ca60bc431873fee6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
